{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T06:32:34.929592Z",
     "start_time": "2020-04-18T06:32:34.922728Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "from pypinyin import lazy_pinyin, Style\n",
    "import os\n",
    "import jieba\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import ShuffleSplit, KFold, train_test_split\n",
    "import operator\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from sklearn.metrics import roc_curve, auc \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T06:32:35.407755Z",
     "start_time": "2020-04-18T06:32:35.393030Z"
    }
   },
   "outputs": [],
   "source": [
    "fname = \"./pinyin_train.txt\"\n",
    "# fname = \"./维基语料.txt\"\n",
    "with open(fname, 'r', encoding=\"UTF-8\") as f:\n",
    "    source = f.read()\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载测试语句（流行词偏多）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['sui',\n",
       "   'ran',\n",
       "   'yi',\n",
       "   'jing',\n",
       "   'jie',\n",
       "   'jue',\n",
       "   'le',\n",
       "   'jian',\n",
       "   'li',\n",
       "   'xin',\n",
       "   'ji',\n",
       "   'fen',\n",
       "   'fang',\n",
       "   'fa',\n",
       "   'de',\n",
       "   'shou',\n",
       "   'yao',\n",
       "   'wen',\n",
       "   'ti'],\n",
       "  '虽然已经解决了建立新积分方法的首要问题'),\n",
       " (['jian',\n",
       "   'li',\n",
       "   'le',\n",
       "   'jiao',\n",
       "   'yi',\n",
       "   'ban',\n",
       "   'ji',\n",
       "   'shang',\n",
       "   'de',\n",
       "   'ce',\n",
       "   'du',\n",
       "   'li',\n",
       "   'lun'],\n",
       "  '建立了较一般集上的测度理论'),\n",
       " (['hou',\n",
       "   'mian',\n",
       "   'wo',\n",
       "   'men',\n",
       "   'jiang',\n",
       "   'cheng',\n",
       "   'ju',\n",
       "   'you',\n",
       "   'zhe',\n",
       "   'zhong',\n",
       "   'xing',\n",
       "   'zhi',\n",
       "   'de',\n",
       "   'han',\n",
       "   'shu',\n",
       "   'wei',\n",
       "   'ke',\n",
       "   'ce',\n",
       "   'han',\n",
       "   'shu'],\n",
       "  '后面我们将称具有这种性质的函数为可测函数'),\n",
       " (['xia', 'mian', 'yin', 'ru', 'ke', 'ce', 'han', 'shu', 'de', 'gai', 'nian'],\n",
       "  '下面引入可测函数的概念'),\n",
       " (['ke', 'ce', 'han', 'shu', 'de', 'you', 'xian', 'ke', 'jia', 'xing'],\n",
       "  '可测函数的有限可加性'),\n",
       " (['ji',\n",
       "   'hu',\n",
       "   'chu',\n",
       "   'chu',\n",
       "   'shou',\n",
       "   'lian',\n",
       "   'han',\n",
       "   'shu',\n",
       "   'lie',\n",
       "   'de',\n",
       "   'kong',\n",
       "   'zhi',\n",
       "   'shou',\n",
       "   'lian',\n",
       "   'ding',\n",
       "   'li'],\n",
       "  '几乎处处收敛函数列的控制收敛定理'),\n",
       " (['zheng',\n",
       "   'ming',\n",
       "   'ji',\n",
       "   'fen',\n",
       "   'yu',\n",
       "   'ji',\n",
       "   'xian',\n",
       "   'jiao',\n",
       "   'huan',\n",
       "   'shun',\n",
       "   'xu'],\n",
       "  '证明积分与极限交换顺序'),\n",
       " (['zai',\n",
       "   'ju',\n",
       "   'yi',\n",
       "   'xie',\n",
       "   'kong',\n",
       "   'zhi',\n",
       "   'shou',\n",
       "   'lian',\n",
       "   'ding',\n",
       "   'li',\n",
       "   'de',\n",
       "   'ying',\n",
       "   'yong'],\n",
       "  '再举一些控制收敛定理的应用'),\n",
       " (['du',\n",
       "   'zhe',\n",
       "   'zi',\n",
       "   'ji',\n",
       "   'ye',\n",
       "   'ke',\n",
       "   'yi',\n",
       "   'lie',\n",
       "   'ju',\n",
       "   'bing',\n",
       "   'jia',\n",
       "   'yi',\n",
       "   'zheng',\n",
       "   'ming'],\n",
       "  '读者自己也可以列举并加以证明'),\n",
       " (['suo',\n",
       "   'yi',\n",
       "   'zhe',\n",
       "   'liang',\n",
       "   'ge',\n",
       "   'han',\n",
       "   'shu',\n",
       "   'ji',\n",
       "   'hu',\n",
       "   'chu',\n",
       "   'chu',\n",
       "   'xiang',\n",
       "   'deng'],\n",
       "  '所以这两个函数几乎处处相等'),\n",
       " (['Jin', 'tian', 'ye', 'shi', 'hao', 'tian', 'qi'], '今天也是好天气')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = \"./test_set.txt\"\n",
    "test_list=[]\n",
    "cnt = 0\n",
    "with open(fname, 'r', encoding=\"UTF-8\") as ft:\n",
    "    for line in ft.readlines():\n",
    "        if cnt%2 == 0:\n",
    "            tmp = []\n",
    "            pinyinlist = line.strip(\"\\n\").split(\" \")\n",
    "            tmp.append(pinyinlist)\n",
    "        else:\n",
    "            tmp.append(line.strip(\"\\n\"))\n",
    "            test_list.append(tuple(tmp))\n",
    "        cnt += 1\n",
    "ft.close()\n",
    "test_list[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:37:05.477553Z",
     "start_time": "2020-04-18T07:37:05.392685Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.155 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'[^\\u4e00-\\u9fa5]')    # 保留中文\n",
    "chinese = re.sub(pattern, '', source)\n",
    "distinct_tags = set(chinese)\n",
    "print(len(distinct_tags))\n",
    "# chinese[:100]\n",
    "# seg_list = list(jieba.cut(chinese, HMM=True))            # 默认是精确模式， 不重复\n",
    "seg_list = list(jieba.cut_for_search(chinese, HMM=True))    # 搜索引擎的模式， 词语更多，重复\n",
    "\n",
    "\n",
    "seg_list_initials = seg_list.copy()\n",
    "# seg_list = jieba.cut_for_search(chinese[:100]) #默认是精确模式\n",
    "\n",
    "# for word in seg_list:\n",
    "#     print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "汉字词组转化为拼音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-18T07:37:37.853291Z",
     "start_time": "2020-04-18T07:37:06.219394Z"
    }
   },
   "outputs": [],
   "source": [
    "# os.environ['PYPINYIN_NO_PHRASES'] = 'true'\n",
    "# os.environ['PYPINYIN_NO_DICT_COPY'] = 'true'\n",
    "# style = Style.NORMAL\n",
    "\n",
    "fname = \"./pinyin_tagger.txt\"\n",
    "with open(fname, 'w', encoding=\"UTF-8\") as fw:\n",
    "    \n",
    "    for word in seg_list:\n",
    "        \n",
    "        fw.write('START/START\\n')\n",
    "        seg_py = lazy_pinyin(word, style=Style.INITIALS, strict=False)\n",
    "        for index, pinyin in enumerate(seg_py):\n",
    "            fw.write(pinyin+'/'+word[index]+'\\n')\n",
    "        fw.write('END/END\\n')\n",
    "        \n",
    "        fw.write('START/START\\n')\n",
    "        seg_py = lazy_pinyin(word, style=Style.NORMAL)\n",
    "        for index, pinyin in enumerate(seg_py):\n",
    "            fw.write(pinyin+'/'+word[index]+'\\n')\n",
    "        fw.write('END/END\\n')\n",
    "#     for word in seg_list_initials:\n",
    "\n",
    "fw.close()\n",
    "\n",
    "# 追加写入\n",
    "# with open(fname, 'a', encoding=\"UTF-8\") as fwi:\n",
    "\n",
    "# fw.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立拼音——词语字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full',)).History will not be written to the database.\n",
      "zh\n",
      "找咫渣战杖正准眨汁种州濯障竺捉债撞骤主掷中\n"
     ]
    }
   ],
   "source": [
    "pinyin2hanzi = np.load( \"./data/py2hanzi.npy\", allow_pickle=True).item() # 拼音汉字映射表\n",
    "pinyin2hanzi[ 'lv' ] = pinyin2hanzi[ 'lü' ]\n",
    "pinyin2hanzi[ 'lue' ] = pinyin2hanzi[ 'lüe' ]\n",
    "pinyin2hanzi[ 'nv' ] = pinyin2hanzi[ 'nü' ]\n",
    "pinyin2hanzi[ 'nue' ] = pinyin2hanzi[ 'nüe' ]\n",
    "# pinyin_initials2hanzi = defaultdict(list)\n",
    "# 追加\n",
    "word_list = list(distinct_tags)\n",
    "ini_py = lazy_pinyin(word_list, style=Style.INITIALS, strict=False)\n",
    "for idx, py in enumerate(ini_py):\n",
    "#     pinyin_initials2hanzi[py].append(word_list[idx])\n",
    "    pinyin2hanzi[py] = pinyin2hanzi.get(py, \"\") + word_list[idx]\n",
    "    if word_list[ idx ] == \"中\":\n",
    "        print(py)\n",
    "        print(pinyin2hanzi['zh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['a', 'ai', 'an', 'ang', 'ao', 'ba', 'bai', 'ban', 'bang', 'bao', 'bei', 'ben', 'beng', 'bi', 'bian', 'biao', 'bie', 'bin', 'bing', 'bo', 'bu', 'ca', 'cai', 'can', 'cang', 'cao', 'ce', 'ceng', 'ci', 'cong', 'cou', 'cu', 'cuan', 'cui', 'cun', 'cuo', 'cha', 'chai', 'chan', 'chang', 'chao', 'che', 'chen', 'cheng', 'chi', 'chong', 'chou', 'chu', 'chuai', 'chuan', 'chuang', 'chui', 'chun', 'chuo', 'da', 'dai', 'dan', 'dang', 'dao', 'de', 'deng', 'di', 'dian', 'diao', 'die', 'ding', 'diu', 'dong', 'dou', 'du', 'duan', 'dui', 'dun', 'duo', 'e', 'en', 'er', 'fa', 'fan', 'fang', 'fei', 'fen', 'feng', 'fo', 'fou', 'fu', 'ga', 'gai', 'gan', 'gang', 'gao', 'ge', 'gei', 'gen', 'geng', 'gong', 'gou', 'gu', 'gua', 'guai', 'guan', 'guang', 'gui', 'gun', 'guo', 'ha', 'hai', 'han', 'hang', 'hao', 'he', 'hei', 'hen', 'heng', 'hong', 'hou', 'hu', 'hua', 'huai', 'huan', 'huang', 'hui', 'hun', 'huo', 'ji', 'jia', 'jian', 'jiang', 'jiao', 'jie', 'jin', 'jing', 'jiong', 'jiu', 'ju', 'juan', 'jue', 'jun', 'ka', 'kai', 'kao', 'kan', 'kang', 'ke', 'ken', 'keng', 'kong', 'kou', 'ku', 'kua', 'kuai', 'kuan', 'kuang', 'kui', 'kun', 'kuo', 'la', 'lai', 'lan', 'lang', 'lao', 'le', 'lei', 'leng', 'li', 'lian', 'liang', 'liao', 'lie', 'lin', 'ling', 'liu', 'lo', 'long', 'lou', 'lu', 'luan', 'lun', 'luo', 'lü', 'lüe', 'ma', 'mai', 'mao', 'man', 'mang', 'me', 'mei', 'men', 'meng', 'mi', 'miao', 'mian', 'mie', 'min', 'ming', 'miu', 'mo', 'mou', 'mu', 'na', 'nai', 'nao', 'nan', 'nang', 'ne', 'nei', 'nen', 'neng', 'ni', 'nian', 'niang', 'niao', 'nie', 'nin', 'ning', 'niu', 'nou', 'nong', 'nu', 'nuan', 'nuo', 'nü', 'nüe', 'o', 'ou', 'pa', 'pai', 'pao', 'pan', 'pang', 'pei', 'pen', 'peng', 'pi', 'piao', 'pian', 'pie', 'pin', 'ping', 'po', 'pou', 'pu', 'qi', 'qia', 'qiao', 'qian', 'qiang', 'qie', 'qin', 'qing', 'qiong', 'qiu', 'qu', 'quan', 'que', 'qun', 'ran', 'rang', 'rao', 're', 'ren', 'reng', 'ri', 'rong', 'rou', 'ru', 'ruan', 'rui', 'run', 'ruo', 'sa', 'sai', 'san', 'sang', 'sao', 'se', 'sen', 'seng', 'si', 'song', 'sou', 'su', 'suan', 'sui', 'sun', 'suo', 'sha', 'shai', 'shan', 'shang', 'shao', 'she', 'shei', 'shen', 'sheng', 'shi', 'shou', 'shu', 'shua', 'shuai', 'shuan', 'shuang', 'shui', 'shun', 'shuo', 'ta', 'tai', 'tan', 'tang', 'tao', 'te', 'teng', 'ti', 'tian', 'tiao', 'tie', 'ting', 'tong', 'tou', 'tu', 'tuan', 'tui', 'tun', 'tuo', 'wa', 'wai', 'wan', 'wang', 'wei', 'wen', 'weng', 'wo', 'wu', 'xi', 'xia', 'xian', 'xiang', 'xiao', 'xie', 'xin', 'xing', 'xiong', 'xiu', 'xu', 'xuan', 'xue', 'xun', 'ya', 'yan', 'yang', 'yao', 'ye', 'yi', 'yin', 'ying', 'yo', 'yong', 'you', 'yu', 'yuan', 'yue', 'yun', 'za', 'zai', 'zan', 'zang', 'zao', 'ze', 'zei', 'zen', 'zeng', 'zi', 'zong', 'zou', 'zu', 'zuan', 'zui', 'zun', 'zuo', 'zha', 'zhai', 'zhan', 'zhang', 'zhao', 'zhe', 'zhei', 'zhen', 'zheng', 'zhi', 'zhong', 'zhou', 'zhu', 'zhua', 'zhuai', 'zhuan', 'zhuang', 'zhui', 'zhun', 'zhuo', 'lv', 'lue', 'nv', 'nue', 'sh', 'd', 'y', 'z', 'l', 'x', 't', 'q', 'zh', 'b', 'j', 'p', 'f', 'w', 'h', 'g', 'k', '', 'n', 'm', 'ch', 'r', 'c', 's'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pinyin_initials2hanzi\n",
    "pinyin2hanzi.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'找咫渣战杖正准眨汁种州濯障竺捉债撞骤主掷中注至蛛炸支祯逐智致兆值震秩锥扎彰丈洲肘涨札郑治助湛崭抓桌赵侦帐徵圳毡直贮诊旨闸煮斩遮蒸嘱赈祝詹著爪止只乍制照诈沼着撰证殖重窄伫烛针昭砖拙周置脂占筑追宅箴杼铸珠粘寨植诸峙峥转坠肢柱吒侄拯真章站辙状珍瞻衷知轴召仲专宙忠执舟卓竹仗铢漳贞众驻踵挣帜株镇沾争挚肇枕症张招稚摺志展帧阵征胀织栅质斋辄镯咒臻址账职政朱赚钟摘妆终罩整滞者浙住疹篆装壮趾祉窒长之轾瞩猪掌枝哲洙桩蛀庄桢振纸肿昼折芝指这'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinyin2hanzi['zh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-18T07:38:30.818Z"
    }
   },
   "outputs": [],
   "source": [
    "fname = \"./pinyin_tagger.txt\"\n",
    "with open(fname, 'r', encoding=\"UTF-8\") as f:\n",
    "    source_tagger = f.read()\n",
    "# source_tagger.strip(\"\\\\ufeff\")        #带签名\n",
    "\n",
    "word_taggers = [nltk.tag.str2tuple(t) for t in source_tagger.split()]\n",
    "tagger_words = [(word_tagger[1], word_tagger[0].lower()) for word_tagger in  word_taggers]       # (现象， 标签) 互换成 (标签， 现象)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_HMM(tagger_words):\n",
    "    # conditional frequency distribution    发射频率 \n",
    "    cfd_tagwords = nltk.ConditionalFreqDist(tagger_words)\n",
    "#     cfdd = cfd_tagwords['t']\n",
    "#     cfdd.plot(10)\n",
    "#     cfdt = cfd_tagwords['tu']\n",
    "#     cfdt.plot(10)\n",
    "    # conditional probability distribution       发射概率\n",
    "    cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, nltk.MLEProbDist)\n",
    "    \n",
    "\n",
    "    treebank_tags = [ tag for (tag, word) in tagger_words ]\n",
    "    # count(t{i-1} ti)\n",
    "    # bigram的意思是 前后两个一组，联在一起\n",
    "    cfd_tags= nltk.ConditionalFreqDist(nltk.bigrams(treebank_tags))\n",
    "    # P(ti | t{i-1})                            转移概率   \n",
    "    cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)\n",
    "    \n",
    "    return cpd_tags, cpd_tagwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cpd_tags, cpd_tagwords) = train_HMM(tagger_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for tag in distinct_tags:\n",
    "#     best = max(tag, key = lambda tag: cpd_tags['START'].prob(tag) )\n",
    "# best\n",
    "# math.log(cpd_tags['START'].prob(best))\n",
    "# cpd_tags['START'].prob(best)\n",
    "# tagger_words[:]\n",
    "# source_tagger[-100:-1]\n",
    "# cpd_tagwords['土'].prob('t')\n",
    "# cpd_tagwords['她'].prob('ta')\n",
    "# cpd_tagwords['地'].prob('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tagger_word = [('a', 'A'), ()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('model_cut_for_search.npy', (cpd_tags, cpd_tagwords, pinyin2hanzi))\n",
    "# 读取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cpd_tags, cpd_tagwords, pinyin2hanzi) = np.load(\"./model_cut_for_search.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 识别函数，参数列表： 转移概率，发射概率，原句，标签集合，已标记的标签\n",
    "def recognize_HMM(cpd_tags, cpd_tagwords, sentence, pinyin2hanzi, labeled_tags = [], DEBUG=False):\n",
    "    viterbi = [ ]           # 维特比链\n",
    "#     backpointer = [ ]       # 回溯器\n",
    "    \n",
    "    first_viterbi = defaultdict(int)\n",
    "    first_backpointer = { }\n",
    "    for tag in pinyin2hanzi[ sentence[1] ]:\n",
    "        # don't record anything for the START tag\n",
    "        # Y 是 第一个拼音 的每一种可能的Tag\n",
    "        # P(Y | \"START\") * P( \"第一个拼音\" | \"Y\")\n",
    "        if tag == \"START\": continue\n",
    "        first_viterbi[ tag ] = cpd_tags[\"START\"].prob(tag) * cpd_tagwords[tag].prob( sentence[1] )\n",
    "        # P( \"第一个拼音\" | \"Y\")\n",
    "#         first_viterbi[ tag ] = cpd_tagwords[tag].prob( sentence[1] ) \n",
    "\n",
    "#         first_backpointer[ tag ] = \"START\"\n",
    "    viterbi.append(first_viterbi)\n",
    "#     backpointer.append(first_backpointer)\n",
    "    \n",
    "    # 这里是 求 (START, END), 因为如果把 \"END\" 也算入，循环之后取出来的概率就是\"END\"的Tag（错误)，而不是\"END\"之前的那个Tag\n",
    "    for wordindex in range(2, len(sentence) - 1):\n",
    "        this_viterbi = defaultdict(int)\n",
    "#         this_backpointer = { }\n",
    "        prev_viterbi = viterbi[-1]\n",
    "        for tag in pinyin2hanzi[ sentence[ wordindex ] ]:\n",
    "            # START没有卵用的，我们要忽略\n",
    "            if tag == \"START\": continue\n",
    "            # 如果现在这个tag是X，现在的单词是w，\n",
    "            # 我们想找前一个tag Y，并且让最好的tag sequence以 Y X 结尾。\n",
    "            # 也就是说\n",
    "            # Y要能最大化：\n",
    "            # prev_viterbi[ Y ] * P(X | Y) * P( w | X)\n",
    "            # 排序——逆序\n",
    "            best_previous_list = sorted(prev_viterbi.keys(),\n",
    "                                key = lambda prevtag: \\\n",
    "                        prev_viterbi[ prevtag ] * cpd_tags[ prevtag[-1] ].logprob(tag) * cpd_tagwords[tag].logprob(sentence[wordindex]),\n",
    "                                       reverse = True)\n",
    "            \n",
    "            # 如果是前缀的话，为了避免状态爆炸，取最大的20个就好了，错误，\n",
    "            \n",
    "            for best_previous in  best_previous_list: \n",
    "                # 不应该是一阶HMM了，应该是全部的前缀  !!!!!!!\n",
    "                prob =  prev_viterbi[ best_previous ] * \\\n",
    "                        cpd_tags[ best_previous[-1] ].prob(tag) * cpd_tagwords[ tag ].prob(sentence[wordindex]) \n",
    "                if prob == 0:\n",
    "                    continue\n",
    "                this_viterbi[ best_previous + tag ] = prob\n",
    "#             this_backpointer[ tag ] = best_previous_list[0]\n",
    "        # 每次遍历Tag集找完Y 我们把目前最好的 X = currbest存一下\n",
    "        if DEBUG:\n",
    "            currbest = max(this_viterbi.keys(), key = lambda tag: this_viterbi[ tag ])\n",
    "#             print( \"Word\", \"'\" + sentence[ wordindex ] + \"'\", \"current best two-tag sequence:\", this_backpointer[currbest], currbest)\n",
    "            print( \"Word\", \"'\" + sentence[ wordindex ] + \"'\", \"current best pre-sequence:\", currbest)\n",
    "        \n",
    "        # 完结\n",
    "        # 全部存下来\n",
    "        viterbi.append(this_viterbi)\n",
    "#         backpointer.append(this_backpointer)\n",
    "    \n",
    "    # 找所有以END结尾的tag sequence\n",
    "    # prev_viterbi[ Y ] * P(\"END\" | Y), Y是“END\"之前的tag, 这里是发射概率\n",
    "    prev_viterbi = viterbi[-1]\n",
    "    \n",
    "#     print(backpointer)\n",
    "    \n",
    "    # 同理 这里也有倒过来 !!!!!!!!!!!!!!!   就放到了循环了。。。\n",
    "#     backpointer.reverse()\n",
    "    \n",
    "    # 取所有概率大于0 的\n",
    "    word_prob_dict = { }\n",
    "    for key in prev_viterbi.keys():\n",
    "        word_prob_dict[ key ] = prev_viterbi[ key ] * cpd_tags[ key[-1] ].prob(\"END\")\n",
    "    \n",
    "    if DEBUG:\n",
    "        for (best_previous, prob_tagsequence) in sorted(word_prob_dict.items(), key = lambda item: item[1], reverse = True)[:20]:\n",
    "            # 就是排序的概率，再算一次\n",
    "    #         prob_tagsequence = prev_viterbi[ best_previous ] * cpd_tags[ best_previous[-1] ].prob(\"END\")    \n",
    "#             prob_tagsequence = best_previous[1]\n",
    "            # 我们这会儿是倒着存的。因为好的在后面\n",
    "    #         best_tagsequence = [ \"END\", best_previous ]\n",
    "\n",
    "            # 回溯 最好的tag\n",
    "    #         current_best_tag = best_previous\n",
    "            # 这里为什么可以把 最后一个 回溯dict忽略？？？？？？\n",
    "    #         for bp in backpointer[:-1]:\n",
    "    #             print(bp)\n",
    "    #             best_tagsequence.append(bp[current_best_tag])\n",
    "    #             current_best_tag = bp[current_best_tag]\n",
    "            # 因为\"START\" \"NNP\" 中 \"NNP\" 总是能在第一个单词的 Y 中最大化\n",
    "    #         best_tagsequence.append(\"START\")\n",
    "    #         best_tagsequence.reverse()\n",
    "\n",
    "            print( \"The tag sequence is:\", best_previous, end = \" \")\n",
    "        #         print( \"The tag sequence is:\", end = \" \")\n",
    "\n",
    "        #         for t in best_tagsequence: print (t, end = \" \")\n",
    "            print( \". The probability of the tag sequence is:\", prob_tagsequence if prob_tagsequence <=0 else math.log(prob_tagsequence))\n",
    "    # 为结果排序\n",
    "    best_previous_list = sorted(word_prob_dict.items(),\n",
    "                           key = lambda item: float(\"-inf\") if item[1] <= 0 else math.log(item[1]),\n",
    "                           reverse = True)\n",
    "    \n",
    "#     best_previous = max(prev_viterbi.keys(),\n",
    "#                         key = lambda prevtag: prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(\"END\"))\n",
    "#     prob_tagsequence = prev_viterbi[ best_previous ] * cpd_tags[ best_previous].prob(\"END\")\n",
    "#     # 我们这会儿是倒着存的。因为好的在后面\n",
    "#     best_tagsequence = [ \"END\", best_previous ]\n",
    "#     # 同理 这里也有倒过来\n",
    "#     backpointer.reverse()\n",
    "#     # 回溯 最好的tag\n",
    "#     current_best_tag = best_previous\n",
    "#     for bp in backpointer[:-1]:\n",
    "#         best_tagsequence.append(bp[current_best_tag])\n",
    "#         current_best_tag = bp[current_best_tag]\n",
    "#     # 因为\"START\" \"NNP\" 中 \"NNP\" 总是能在第一个单词的 Y 中最大化\n",
    "#     best_tagsequence.append(\"START\")\n",
    "#     best_tagsequence.reverse()\n",
    "\n",
    "    if DEBUG:\n",
    "\n",
    "        print( \"\\nThe sentence was:    \", end = \" \")\n",
    "        for w in sentence: print( w, end = \" \")\n",
    "\n",
    "        print( \".\\nThe labeled tag sequence is:\", end = \" \")\n",
    "        for l in labeled_tags: print(l, end = \" \")\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        \n",
    "#     return best_tagsequence\n",
    "    return best_previous_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('START', 'start'),\n",
       " ('中', 'zh'),\n",
       " ('国', 'g'),\n",
       " ('END', 'end'),\n",
       " ('START', 'start'),\n",
       " ('中', 'zhong'),\n",
       " ('国', 'guo'),\n",
       " ('END', 'end'),\n",
       " ('START', 'start'),\n",
       " ('十', 'sh')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c', 'sh']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lazy_pinyin(\"测试\", style=Style.INITIALS, strict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'h']\n",
      "Word 'h' current best pre-sequence: 南韩\n",
      "The tag sequence is: 南韩 . The probability of the tag sequence is: -10.600948731576649\n",
      "The tag sequence is: 女孩 . The probability of the tag sequence is: -12.845142258993079\n",
      "The tag sequence is: 脑会 . The probability of the tag sequence is: -13.155531780404212\n",
      "The tag sequence is: 南海 . The probability of the tag sequence is: -13.322440718142314\n",
      "The tag sequence is: 暖和 . The probability of the tag sequence is: -13.497749020915373\n",
      "The tag sequence is: 男孩 . The probability of the tag sequence is: -13.665588414494959\n",
      "The tag sequence is: 奈何 . The probability of the tag sequence is: -13.875631843349202\n",
      "The tag sequence is: 尼黑 . The probability of the tag sequence is: -14.342808232428895\n",
      "The tag sequence is: 霓虹 . The probability of the tag sequence is: -14.350843079515734\n",
      "The tag sequence is: 暖化 . The probability of the tag sequence is: -14.359738715918562\n",
      "The tag sequence is: 年会 . The probability of the tag sequence is: -14.43314137230411\n",
      "The tag sequence is: 南汇 . The probability of the tag sequence is: -14.551413876455916\n",
      "The tag sequence is: 男花 . The probability of the tag sequence is: -14.839844746492417\n",
      "The tag sequence is: 浓厚 . The probability of the tag sequence is: -14.923937075884757\n",
      "The tag sequence is: 内讧 . The probability of the tag sequence is: -14.937166599971516\n",
      "The tag sequence is: 内涵 . The probability of the tag sequence is: -15.59641222885578\n",
      "The tag sequence is: 纳赫 . The probability of the tag sequence is: -15.617567094405144\n",
      "The tag sequence is: 南宏 . The probability of the tag sequence is: -15.752315155769425\n",
      "The tag sequence is: 尼赫 . The probability of the tag sequence is: -15.942712471382134\n",
      "The tag sequence is: 怒火 . The probability of the tag sequence is: -16.376777467148\n",
      "\n",
      "The sentence was:     START n h END .\n",
      "The labeled tag sequence is: 建 立 了 较 一 般 集 上 的 测 度 理 论 \n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# distinct_tags = set([tag for (tag, word) in tagger_words ])\n",
    "\n",
    "test_array = np.array(test_list)\n",
    "index = np.random.choice(len(test_array), size = 1)\n",
    "test_sentence = test_array[index][0]\n",
    "Sample_sentence, Sample_tag = test_sentence[0], test_sentence[1]\n",
    "# Sample_tag = test_sentence[1]\n",
    "# 声母测试\n",
    "Sample_sentence = list(lazy_pinyin(Sample_tag, style=Style.INITIALS, strict=False))\n",
    "# 自定义测试\n",
    "# Sample_sentence = ['ni', 'hao', 'sao', 'ya']\n",
    "Sample_sentence = ['n', 'h']\n",
    "\n",
    "# 通过分词模拟拼音输入\n",
    "# tokens = jieba.tokenize(Sample_tag, HMM=False)\n",
    "# for tk in tokens:\n",
    "#     tags = tk[0]\n",
    "#     sentence = ['START'] + list(map(lambda x : x.lower(), Sample_sentence[tk[1]: tk[2]])) + ['END']\n",
    "\n",
    "#     _ = recognize_HMM(cpd_tags, cpd_tagwords, sentence, pinyin2hanzi, tags, DEBUG=True)\n",
    "\n",
    "# 整句输入， 注意 前后加上 \"START\" 和 \"END\"\n",
    "sentence, tags = Sample_sentence, Sample_tag\n",
    "print(Sample_sentence)\n",
    "sentence = [\"START\"] + list(map(lambda x : x.lower(), sentence)) + [\"END\"]\n",
    "_ = recognize_HMM(cpd_tags, cpd_tagwords, sentence, pinyin2hanzi, tags, DEBUG=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_fname = './pinyin_test.txt'\n",
    "with open(test_fname, 'r', encoding=\"UTF-8\") as ft:\n",
    "    test_text = ft.read()\n",
    "sum_hits = 0\n",
    "no_alpha = re.compile('[^a-zA-Z ]')\n",
    "while True:\n",
    "    sentence = input()\n",
    "    if sentence == '': break\n",
    "    if no_alpha.match(sentence) != None:\n",
    "        continue\n",
    "    # 统计输入长度 + 回车键 + 空格\n",
    "    input_hits = len(sentence) + 1\n",
    "    \n",
    "    sentence = [\"START\"] + sentence.strip(\" \").split(\" \") + [\"END\"]\n",
    "#     print(sentence)\n",
    "#     看一下是哪个词不符合规范了\n",
    "    flag = False\n",
    "    for word in sentence[1: -1]:\n",
    "        if word not in pinyin2hanzi.keys():\n",
    "            print(\"The pinyin\", word, 'is invalid!!!! Please input again!')\n",
    "            flag = True\n",
    "    if flag:\n",
    "        continue\n",
    "            \n",
    "    res_list =  recognize_HMM(cpd_tags, cpd_tagwords, sentence, pinyin2hanzi, tags, DEBUG=False)    \n",
    "    print(res_list)\n",
    "##    偷懒测试 过于 傻逼\n",
    "#     for index, word in enumerate(res_list):\n",
    "#         if word[0] in test_text:\n",
    "#             sum_hits += (index+1)//10+1\n",
    "#             sum_hits += input_hits\n",
    "#             print(\"Find word\", word, \"in position\", index, \", which will takes\", (index+1)//10+1, \"times hits.\")\n",
    "#             break\n",
    "\n",
    "#     else:\n",
    "#         print(\"We can't find the target word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 3), match='fai'>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinyin_pattern = re.compile(r\"[^aoeiuv]?h?[iuv]?(ai|ei|ao|ou|er|ang?|eng?|ong|a|o|e|i|u|ng|n)?\")\n",
    "pinyin_pattern = re.compile(r\"\\\n",
    "([mM]iu|[pmPM]ou|[bpmBPM](o|e(i|ng?)?|a(ng?|i|o)?|i(e|ng?|a[no])?|u))|\\\n",
    "([fF](ou?|[ae](ng?|i)?|u))|([dD](e(i|ng?)|i(a[on]?|u))|\\\n",
    "[dtDT](a(i|ng?|o)?|e(i|ng)?|i(a[on]?|e|ng|u)?|o(ng?|u)|u(o|i|an?|n)?))|\\\n",
    "([nN]eng?|[lnLN](a(i|ng?|o)?|e(i|ng)?|i(ang|a[on]?|e|ng?|u)?|o(ng?|u)|u(o|i|an?|n)?|ve?))|\\\n",
    "([ghkGHK](a(i|ng?|o)?|e(i|ng?)?|o(u|ng)|u(a(i|ng?)?|i|n|o)?))|\\\n",
    "([zZ]h?ei|[czCZ]h?(e(ng?)?|o(ng?|u)?|ao|u?a(i|ng?)?|u?(o|i|n)?))|\\\n",
    "([sS]ong|[sS]hua(i|ng?)?|[sS]hei|[sS][h]?(a(i|ng?|o)?|en?g?|ou|u(a?n|o|i)?|i))|\\\n",
    "([rR]([ae]ng?|i|e|ao|ou|ong|u[oin]|ua?n?))|\\\n",
    "([jqxJQX](i(a(o|ng?)?|[eu]|ong|ng?)?|u(e|a?n)?))|\\\n",
    "(([aA](i|o|ng?)?|[oO]u?|[eE](i|ng?|r)?))|\\\n",
    "([wW](a(i|ng?)?|o|e(i|ng?)?|u))|\\\n",
    "[yY](a(o|ng?)?|e|in?g?|o(u|ng)?|u(e|a?n)?)\\\n",
    "\")\n",
    "# pinyin_pattern = re.compile(r\"(a[io]?|ou?|e[inr]?|ang?|ng|[bmp](a[io]?|[aei]ng?|ei|ie?|ia[no]|o|u)|pou|me|m[io]u|[fw](a|[ae]ng?|ei|o|u)|fou|wai|[dt](a[io]?|an|e|[aeio]ng|ie?|ia[no]|ou|u[ino]?|uan)|dei|diu|[nl](a[io]?|ei?|[eio]ng|i[eu]?|i?ang?|iao|in|ou|u[eo]?|ve?|uan)|nen|lia|lun|[ghk](a[io]?|[ae]ng?|e|ong|ou|u[aino]?|uai|uang?)|[gh]ei|[jqx](i(ao?|ang?|e|ng?|ong|u)?|u[en]?|uan)|([csz]h?|r)([ae]ng?|ao|e|i|ou|u[ino]?|uan)|[csz](ai?|ong)|[csz]h(ai?|uai|uang)|zei|[sz]hua|([cz]h|r)ong|y(ao?|[ai]ng?|e|i|ong|ou|u[en]?|uan))\")\n",
    "\n",
    "pinyin_pattern.match(r\"fai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpd_tagwords['都'].prob('d')\n",
    "cpd_tags['国'].prob('美')\n",
    "_\n",
    "sum_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "if '新华社' in test_text:\n",
    "    print(\"ok\")\n",
    "no_alpha.fullmatch(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [1, 1, 2, 4]\n",
    "l[1 : -1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
