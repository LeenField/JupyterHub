{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:10:55.210256Z",
     "start_time": "2020-04-01T13:10:55.203448Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from sklearn.model_selection import ShuffleSplit, KFold, train_test_split\n",
    "import operator\n",
    "from math import log\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from sklearn.metrics import roc_curve, auc \n",
    "from sklearn import metrics\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T04:16:14.595984Z",
     "start_time": "2020-03-31T04:16:14.121711Z"
    }
   },
   "outputs": [],
   "source": [
    "fname = \"./ptb.txt\"\n",
    "with open(fname, 'r', encoding=\"UTF-8-sig\") as f:\n",
    "    source_tagger = f.read()\n",
    "# source_tagger.strip(\"\\\\ufeff\")        #带签名\n",
    "\n",
    "tagger_words = [nltk.tag.str2tuple(t) for t in source_tagger.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T04:16:15.035391Z",
     "start_time": "2020-03-31T04:16:14.597772Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({(',', ','): 22625, ('the', 'DT'): 19528, ('.', '.'): 18238, ('of', 'IN'): 10752, ('to', 'TO'): 10408, ('a', 'DT'): 9072, ('and', 'CC'): 7679, ('in', 'IN'): 7040, (\"'s\", 'POS'): 3862, ('for', 'IN'): 3764, ...})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = nltk.FreqDist(tagger_words)\n",
    "fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "介词在介词短语中是两个实质性结构成分中（介词+名词性词语）的一个，而连词在实质性结构中只起连接作用\n",
    " > 当“和”作介词的时候,“和”前后的成分不能互换,前面可加副词作状语,后面可以有停顿\n",
    " \n",
    " > 当“和”作连词的时候,“和”前后的成分可以互换,前面不能加副词性修饰成分,后面不能停顿.\n",
    "\n",
    "In 作为介词In-prep, 连词In-conj:\n",
    "#### 规则：\n",
    "如果下一次词性标记的前两个字母位 NN， 即后面为名词 , IN为介词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T06:37:39.118199Z",
     "start_time": "2020-04-01T06:37:38.894859Z"
    }
   },
   "outputs": [],
   "source": [
    "tagger_words\n",
    "tagger_words_IN = []\n",
    "NN_dict = {'NN', 'NNP', 'NNPS', 'NNS'}\n",
    "for index, (word, tag) in enumerate(tagger_words[:-1]):\n",
    "    if tag == \"IN\":\n",
    "        # 如果下一次词性标记的前两个字母位 NN， 即后面为名词 , IN为介词\n",
    "        if tagger_words[index + 1][1][:2] == 'NN': \n",
    "            tagger_words_IN.append( (word, 'IN-prep') )\n",
    "        # 否则IN为 连词\n",
    "        else:\n",
    "            tagger_words_IN.append( (word, 'IN-conj') )\n",
    "        continue\n",
    "    tagger_words_IN.append( (word, tag) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T07:12:14.693746Z",
     "start_time": "2020-03-31T07:10:15.951126Z"
    }
   },
   "outputs": [],
   "source": [
    "def addStartEnd(tagger_words_list):\n",
    "    tagger_words_SE = []\n",
    "    NewSentence = True\n",
    "    for index, (word, tag) in enumerate(tagger_words_list):\n",
    "        if NewSentence == True:\n",
    "            tagger_words_SE.append([])\n",
    "            tagger_words_SE[-1].append( (\"START\", \"START\") )\n",
    "            NewSentence = False\n",
    "        tagger_words_SE[-1].append( (tag, word) )                # 注意这里是 （标签， 单词）\n",
    "\n",
    "        if word == '.' and tag == '.':\n",
    "            tagger_words_SE[-1].append( (\"END\", \"END\") )\n",
    "            NewSentence = True\n",
    "    return tagger_words_SE\n",
    "\n",
    "tagger_words_sents = addStartEnd(tagger_words)\n",
    "tagger_words_IN_sents = addStartEnd(tagger_words_IN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T07:20:57.343089Z",
     "start_time": "2020-03-31T07:20:01.274682Z"
    }
   },
   "outputs": [],
   "source": [
    "# tagger_words_total = sum(tagger_words_sents, [])\n",
    "tagger_words_total = reduce(operator.add, tagger_words_sents)\n",
    "tagger_words_IN_total = reduce(operator.add, tagger_words_IN_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词性**tag**是**隐含状态**，单词是**观测**\n",
    "\n",
    "1. 首先训练得到HMM模型，即Training问题（不过这里的Training不必用BW算法，直接根据语料库，进行数量的统计和概率的计算就可以完成对转移矩阵和发射矩阵的训练)\n",
    "2. 然后可利用模型，进行词性标注，即Recognition问题，使用**Viterbi算法**即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "某个POS发射出某个单词的概率，即观测(**发射**)概率矩阵B, 大小为$(w, t)$。只是简单粗暴地统计出单词和POS之间的个数关系即可。$$P(w_i|t_i)=\\frac {count(w_i,t_i)}{count(t_i)}$$\n",
    "\n",
    "还需要计算**状态转移矩阵A**, 大小为$(t, t)$，这个只要简单粗暴地计算出由某个状态转移到某个状态的概率即可。\n",
    "$$P(t_i|t_{i−1})= \\frac {count(t_{i−1},t_i)} {count(t_{i−1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现维特比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T08:05:59.520330Z",
     "start_time": "2020-03-31T08:05:59.430236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_tags = set([tag for (tag, word) in tagger_words_total ])\n",
    "distinct_tags_IN = set([tag for (tag, word) in tagger_words_total ])\n",
    "# 一共有 47个标签（IN不分开）\n",
    "len(distinct_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T12:16:10.618692Z",
     "start_time": "2020-04-01T12:16:10.599955Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 识别函数，参数列表： 转移概率，发射概率，原句，标签集合，已标记的标签\n",
    "def recognize_HMM(cpd_tags, cpd_tagwords, sentence, distinct_tags, labeled_tags = [], DEBUG=False):\n",
    "    viterbi = [ ]           # 维特比链\n",
    "    backpointer = [ ]       # 回溯器\n",
    "    \n",
    "    first_viterbi = { }\n",
    "    first_backpointer = { }\n",
    "    for tag in distinct_tags:\n",
    "        # don't record anything for the START tag\n",
    "        # Y 是 第一个单词 的每一种可能的Tag\n",
    "        # P(Y | \"START\") * P( \"第一个单词\" | \"Y\")\n",
    "        if tag == \"START\": continue\n",
    "        first_viterbi[ tag ] = cpd_tags[\"START\"].prob(tag) * cpd_tagwords[tag].prob( sentence[1] )\n",
    "        first_backpointer[ tag ] = \"START\"\n",
    "    viterbi.append(first_viterbi)\n",
    "    backpointer.append(first_backpointer)\n",
    "    \n",
    "    # 这里是 求 (START, END), 因为如果把 \"END\" 也算入，循环之后取出来的概率就是\"END\"的Tag（错误)，而不是\"END\"之前的那个Tag\n",
    "    for wordindex in range(2, len(sentence) - 1):\n",
    "        this_viterbi = { }\n",
    "        this_backpointer = { }\n",
    "        prev_viterbi = viterbi[-1]\n",
    "        for tag in distinct_tags:\n",
    "            # START没有卵用的，我们要忽略\n",
    "            if tag == \"START\": continue\n",
    "            # 如果现在这个tag是X，现在的单词是w，\n",
    "            # 我们想找前一个tag Y，并且让最好的tag sequence以 Y X 结尾。\n",
    "            # 也就是说\n",
    "            # Y要能最大化：\n",
    "            # prev_viterbi[ Y ] * P(X | Y) * P( w | X)\n",
    "            best_previous = max(prev_viterbi.keys(),\n",
    "                                key = lambda prevtag: \\\n",
    "                        prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(tag) * cpd_tagwords[tag].prob(sentence[wordindex]))\n",
    "            this_viterbi[ tag ] = prev_viterbi[ best_previous ] * \\\n",
    "                        cpd_tags[ best_previous ].prob(tag) * cpd_tagwords[ tag].prob(sentence[wordindex])\n",
    "            this_backpointer[ tag ] = best_previous\n",
    "        # 每次遍历Tag集找完Y 我们把目前最好的 X = currbest存一下\n",
    "        currbest = max(this_viterbi.keys(), key = lambda tag: this_viterbi[ tag ])\n",
    "        if DEBUG:\n",
    "            print( \"Word\", \"'\" + sentence[ wordindex] + \"'\", \"current best two-tag sequence:\", this_backpointer[currbest], currbest)\n",
    "        # 完结\n",
    "        # 全部存下来\n",
    "        viterbi.append(this_viterbi)\n",
    "        backpointer.append(this_backpointer)\n",
    "    \n",
    "    # 找所有以END结尾的tag sequence\n",
    "    # prev_viterbi[ Y ] * P(\"END\" | Y), Y是“END\"之前的tag, 这里是发射概率\n",
    "    prev_viterbi = viterbi[-1]\n",
    "    best_previous = max(prev_viterbi.keys(),\n",
    "                        key = lambda prevtag: prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(\"END\"))\n",
    "    prob_tagsequence = prev_viterbi[ best_previous ] * cpd_tags[ best_previous].prob(\"END\")\n",
    "    # 我们这会儿是倒着存的。因为好的在后面\n",
    "    best_tagsequence = [ \"END\", best_previous ]\n",
    "    # 同理 这里也有倒过来\n",
    "    backpointer.reverse()\n",
    "    # 回溯 最好的tag\n",
    "    current_best_tag = best_previous\n",
    "    for bp in backpointer[:-1]:\n",
    "        best_tagsequence.append(bp[current_best_tag])\n",
    "        current_best_tag = bp[current_best_tag]\n",
    "    # 因为\"START\" \"NNP\" 中 \"NNP\" 总是能在第一个单词的 Y 中最大化\n",
    "    best_tagsequence.append(\"START\")\n",
    "    best_tagsequence.reverse()\n",
    "\n",
    "    if DEBUG:\n",
    "        print( \"The sentence was:\", end = \" \")\n",
    "        for w in sentence: print( w, end = \" \")\n",
    "        print(\"\\n\")\n",
    "        print( \"The best tag sequence is:\", end = \" \")\n",
    "        for t in best_tagsequence: print (t, end = \" \")\n",
    "        print(\"\\n\")\n",
    "        print( \"The labeled tag sequence is:\", end = \" \")\n",
    "        for l in labeled_tags: print(l, end = \" \")\n",
    "        print(\"\\n\")\n",
    "        print( \"The probability of the best tag sequence is:\", prob_tagsequence)\n",
    "        \n",
    "    return best_tagsequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先用一句话测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = { \"1\":1, \"2\":2 }\n",
    "sum(d.values())\n",
    "cpd_tags[\" IN\" ].prob( \"IN\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 识别函数，参数列表： 转移概率，发射概率，原句，标签集合，已标记的标签\n",
    "def recognize_HMM(cpd_tags, cpd_tagwords, sentence, distinct_tags, labeled_tags = [], DEBUG=False):\n",
    "    viterbi = [ ]           # 维特比链\n",
    "    backpointer = [ ]       # 回溯器\n",
    "    \n",
    "    first_viterbi = defaultdict(lambda : 0)\n",
    "#     first_backpointer = defaultdict(lambda : \"START\")\n",
    "    first_backpointer = {}\n",
    "    for tag in distinct_tags:\n",
    "        # don't record anything for the START tag\n",
    "        # Y 是 第一个拼音 的每一种可能的Tag\n",
    "        # P(Y | \"START\") * P( \"第一个拼音\" | \"Y\")           XXXX    只需要  P( \"第一个拼音\" | \"Y\") 这个发射概率\n",
    "        if tag == \"START\": continue\n",
    "#         first_viterbi[ tag ] = cpd_tags[\"START\"].logprob(tag) * cpd_tagwords[tag].logprob( sentence[1] ) \n",
    "        first_viterbi[ tag ] = cpd_tagwords[ tag ].prob( sentence[ 1 ] ) \n",
    "    \n",
    "        # 取自然对数\n",
    "#         if first_viterbi[ tag ] > 0:\n",
    "#             first_viterbi[ tag ] = math.log( first_viterbi[ tag ])\n",
    "        first_backpointer[ tag ] = \"START\"\n",
    "    \n",
    "    # 真正的对数化\n",
    "#     sum_viterbi = sum( first_viterbi.values() )\n",
    "#     if sum_viterbi > 0:\n",
    "#         for tag in distinct_tags:\n",
    "#             if tag == \"START\": continue\n",
    "#     #         if first_viterbi[ tag ] > 0:\n",
    "#             first_viterbi[ tag ] = first_viterbi[ tag ] / sum_viterbi\n",
    "        \n",
    "    viterbi.append(first_viterbi)\n",
    "    backpointer.append(first_backpointer)\n",
    "    \n",
    "    # 这里是 求 (START, END), 因为如果把 \"END\" 也算入，循环之后取出来的概率就是\"END\"的Tag（错误)，而不是\"END\"之前的那个Tag\n",
    "    for wordindex in range(2, len(sentence) - 1):\n",
    "        this_viterbi = { }\n",
    "        this_backpointer = { }\n",
    "        prev_viterbi = viterbi[-1]\n",
    "        for tag in distinct_tags:\n",
    "            # START没有卵用的，我们要忽略\n",
    "            if tag == \"START\": continue\n",
    "            # 如果现在这个tag是X，现在的单词是w，\n",
    "            # 我们想找前一个tag Y，并且让最好的tag sequence以 Y X 结尾。\n",
    "            # 也就是说\n",
    "            # Y要能最大化：\n",
    "            # prev_viterbi[ Y ] * P(X | Y) * P( w | X)\n",
    "#             best_previous = max(prev_viterbi.keys(),\n",
    "#                                 key = lambda prevtag: \\\n",
    "#                         prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(tag) * cpd_tagwords[tag].prob(sentence[wordindex]))\n",
    "            best_previous = max(prev_viterbi.keys(),\n",
    "                                key = lambda prevtag: \\\n",
    "                        prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(tag) * cpd_tagwords[tag].prob(sentence[wordindex]))\n",
    "            # 取自然对数，否则概率为0 无法比较\n",
    "            this_viterbi[ tag ] = prev_viterbi[ best_previous ] * \\\n",
    "                        cpd_tags[ best_previous ].prob(tag) * cpd_tagwords[ tag].prob(sentence[wordindex]) \n",
    "#             if this_viterbi[ tag ] > 0:\n",
    "#                 this_viterbi[ tag ] = math.log( this_viterbi[ tag ])\n",
    "            this_backpointer[ tag ] = best_previous\n",
    "    \n",
    "        # 对数化\n",
    "#         sum_viterbi = sum( this_viterbi.values() )\n",
    "#         if sum_viterbi > 0:\n",
    "#             for tag in distinct_tags:\n",
    "#                 if tag == \"START\": continue\n",
    "#     #             if this_viterbi[ tag ] > 0:\n",
    "#                 this_viterbi[ tag ] = this_viterbi[ tag ] / sum_viterbi\n",
    "\n",
    "        \n",
    "        # 每次遍历Tag集找完Y 我们把目前最好的 X = currbest存一下\n",
    "        currbest = max(this_viterbi.keys(), key = lambda tag: this_viterbi[ tag ])\n",
    "        if DEBUG:\n",
    "            print( \"Word\", \"'\" + sentence[ wordindex] + \"'\", \"current best two-tag sequence:\", this_backpointer[currbest], currbest)\n",
    "        # 完结\n",
    "        # 全部存下来\n",
    "        viterbi.append(this_viterbi)\n",
    "        backpointer.append(this_backpointer)\n",
    "    \n",
    "    # 找所有以END结尾的tag sequence\n",
    "    # prev_viterbi[ Y ] * P(\"END\" | Y), Y是“END\"之前的tag, 这里是发射概率\n",
    "    prev_viterbi = viterbi[-1]\n",
    "    best_previous = max(prev_viterbi.keys(),\n",
    "                        key = lambda prevtag: prev_viterbi[ prevtag ] * cpd_tags[prevtag].prob(\"END\"))\n",
    "    prob_tagsequence = prev_viterbi[ best_previous ] * cpd_tags[ best_previous].prob(\"END\")\n",
    "    # 我们这会儿是倒着存的。因为好的在后面\n",
    "    best_tagsequence = [ \"END\", best_previous ]\n",
    "    # 同理 这里也有倒过来\n",
    "    backpointer.reverse()\n",
    "    # 回溯 最好的tag\n",
    "    current_best_tag = best_previous\n",
    "    for bp in backpointer[:-1]:\n",
    "        best_tagsequence.append(bp[current_best_tag])\n",
    "        current_best_tag = bp[current_best_tag]\n",
    "    # 因为\"START\" \"NNP\" 中 \"NNP\" 总是能在第一个单词的 Y 中最大化\n",
    "    best_tagsequence.append(\"START\")\n",
    "    best_tagsequence.reverse()\n",
    "\n",
    "    if DEBUG:\n",
    "        print( \"The sentence was:\", end = \" \")\n",
    "        for w in sentence: print( w, end = \" \")\n",
    "        print(\"\\n\")\n",
    "        print( \"The best tag sequence is:\", end = \" \")\n",
    "        for t in best_tagsequence: print (t, end = \" \")\n",
    "        print(\"\\n\")\n",
    "        print( \"The labeled tag sequence is:\", end = \" \")\n",
    "        for l in labeled_tags: print(l, end = \" \")\n",
    "        print(\"\\n\")\n",
    "        print( \"The probability of the best tag sequence is:\", prob_tagsequence)\n",
    "        \n",
    "    return best_tagsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T06:52:05.715067Z",
     "start_time": "2020-04-01T06:52:05.708029Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_HMM(tagger_words):\n",
    "    # conditional frequency distribution  \n",
    "    cfd_tagwords = nltk.ConditionalFreqDist(tagger_words)\n",
    "    # conditional probability distribution       发射概率\n",
    "    cpd_tagwords = nltk.ConditionalProbDist(cfd_tagwords, nltk.MLEProbDist)\n",
    "#     cpd_tagwords = nltk.DictionaryProbDist(cpd_tagwords, log=True)\n",
    "\n",
    "    treebank_tags = [ tag for (tag, word) in tagger_words ]\n",
    "    # count(t{i-1} ti)\n",
    "    # bigram的意思是 前后两个一组，联在一起\n",
    "    cfd_tags= nltk.ConditionalFreqDist(nltk.bigrams(treebank_tags))\n",
    "    # P(ti | t{i-1})                            转移概率   \n",
    "    cpd_tags = nltk.ConditionalProbDist(cfd_tags, nltk.MLEProbDist)\n",
    "#     cpd_tags = nltk.DictionaryProbDist(cpd_tags, log=True)\n",
    "    \n",
    "    return cpd_tags, cpd_tagwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpd_tags[\"IN\"].logprob(\"IN\")\n",
    "(cpd_tags, cpd_tagwords) = train_HMM(tagger_words_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T12:16:14.582346Z",
     "start_time": "2020-04-01T12:16:13.372871Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 'do' current best two-tag sequence: VBZ VBZ\n",
      "Word 'n't' current best two-tag sequence: VBZ VBZ\n",
      "Word 'want' current best two-tag sequence: VBZ VBZ\n",
      "Word 'companies' current best two-tag sequence: VBZ VBZ\n",
      "Word 'to' current best two-tag sequence: VBZ VBZ\n",
      "Word 'be' current best two-tag sequence: VBZ VBZ\n",
      "Word 'built' current best two-tag sequence: VBZ VBZ\n",
      "Word 'around' current best two-tag sequence: VBZ VBZ\n",
      "Word 'me' current best two-tag sequence: VBZ VBZ\n",
      "Word 'as' current best two-tag sequence: VBZ VBZ\n",
      "Word 'a' current best two-tag sequence: VBZ VBZ\n",
      "Word 'person' current best two-tag sequence: VBZ VBZ\n",
      "Word '.' current best two-tag sequence: VBZ VBZ\n",
      "The sentence was: START I do n't want companies to be built around me as a person . END \n",
      "\n",
      "The best tag sequence is: START VBZ VBZ VBZ VBZ VBZ VBZ VBZ VBZ VBZ VBZ VBZ VBZ VBZ VBZ END \n",
      "\n",
      "The labeled tag sequence is: START PRP VBP RB VB NNS TO VB VBN IN PRP IN DT NN . END \n",
      "\n",
      "The probability of the best tag sequence is: -inf\n"
     ]
    }
   ],
   "source": [
    "tagger_words_array = np.array(tagger_words_sents)           # 一定要array\n",
    "\n",
    "Sample = np.random.choice(tagger_words_array, size = 1)[0]\n",
    "Sample_sentence = [word for (tag, word) in Sample]\n",
    "Sample_tags = [tag for (tag, word) in Sample]\n",
    "_ = recognize_HMM(cpd_tags, cpd_tagwords, Sample_sentence, distinct_tags, Sample_tags, DEBUG=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | 观察结果T | 观察结果F |\n",
    "| --- | --- | --- |\n",
    "| 实际结果T | True Positive | False Positive |\n",
    "| 实际结果F | False Negative | True Negative|\n",
    "\n",
    "定义准确率$precision$，召回率$recall$:\n",
    "$$precision = \\frac{tp} {tp + fp}$$\n",
    "$$recall = \\frac{tp} {tp + fn}$$\n",
    "\n",
    "但是这里的47个tag集合并不是二分类问题，所以我使用 ***sklearn.metrics.precision_score()*** 来计算准确率，这里的多标签需要一个参数：\n",
    "> **average** : string, [None, ‘binary’ (default), ‘micro’, ‘macro’, ‘samples’, ‘weighted’]\n",
    "\n",
    "> This parameter is required for multiclass/multilabel targets. If None, the \n",
    "scores for each class are returned. Otherwise, this\n",
    "determines the type of averaging performed on the data:\n",
    "\n",
    "另外一种计算方法假设标准答案标注的个数是$N$，你的模型给出结果和标准答案相同的分词个数是$c$，你的模型给出的其他单词个数是$c$，也就是标错的个数，那么：\n",
    "$$precision = \\frac{c} {c + e}$$\n",
    "$$recall = \\frac{c} {N}$$\n",
    "$$F_1 = \\frac {2 * P * R}{P + R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练并且测试**不带IN-prep,IN-conj**的标记集，分四次交叉验证，训练集：测试集$ = 9 : 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:17:16.104369Z",
     "start_time": "2020-04-01T13:16:16.133745Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stu3/.local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "/home/stu3/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_score: 0.8307542190003039\n",
      "Recell_score: 0.4990927959270452\n",
      "Precision_score: 0.8201363382987837\n",
      "Recell_score: 0.4605536037371405\n",
      "Precision_score: 0.8605353133423356\n",
      "Recell_score: 0.5212756230317562\n",
      "Precision_score: 0.8394670065801401\n",
      "Recell_score: 0.5029218544492381\n"
     ]
    }
   ],
   "source": [
    "# for train_index, test_index in ss.split(tagger_words_sents):\n",
    "#     print('train_index', train_index, 'test_index', test_index)\n",
    "# #     train_set = tagger_words_sents[list(train_index)]\n",
    "kf = KFold(n_splits=10, random_state=0)     \n",
    "ss = ShuffleSplit(n_splits=4, random_state=0, test_size=0.1)\n",
    "\n",
    "for train_index, test_index in ss.split(tagger_words_array):\n",
    "    train_set, test_set = tagger_words_array[train_index], tagger_words_array[test_index] \n",
    "    train_total = reduce(operator.add, train_set)\n",
    "    cpd_tags, cpd_tagwords = train_HMM(train_total)\n",
    "    true_tags, pred_tags = [], []\n",
    "    for test in test_set:\n",
    "        sentence = [ word for (tag, word) in test ] \n",
    "        labeled_tag = [ tag for (tag, word) in test]\n",
    "        pred_tagsequence = recognize_HMM(cpd_tags, cpd_tagwords, sentence, distinct_tags)   \n",
    "        pred_tags.extend(pred_tagsequence)\n",
    "        true_tags.extend(labeled_tag)\n",
    "        \n",
    "\n",
    "    print(\"Precision_score:\", metrics.precision_score(true_tags, pred_tags, average='macro'))\n",
    "    print(\"Recell_score:\", metrics.recall_score(true_tags, pred_tags, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样训练并且测试**带IN-prep,IN-conj**的标记集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stu3/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_score: 0.749343022298273\n",
      "Recell_score: 0.3370602896190849\n",
      "F1_score: 0.4649723971738213\n",
      "Precision_score: 0.7399626601775345\n",
      "Recell_score: 0.32607049293336027\n",
      "F1_score: 0.4526688286424614\n",
      "Precision_score: 0.7605907477905459\n",
      "Recell_score: 0.36391609211102033\n",
      "F1_score: 0.4922890689681586\n",
      "Precision_score: 0.7285994937792246\n",
      "Recell_score: 0.34363141151554255\n",
      "F1_score: 0.4670070061225022\n"
     ]
    }
   ],
   "source": [
    "def test_HMM(tagger_words_array, distinct_tags):\n",
    "    ss = ShuffleSplit(n_splits=4, random_state=0, test_size=0.1)\n",
    "\n",
    "    for train_index, test_index in ss.split(tagger_words_array):\n",
    "        train_set, test_set = tagger_words_array[train_index], tagger_words_array[test_index] \n",
    "        train_total = reduce(operator.add, train_set)\n",
    "        cpd_tags, cpd_tagwords = train_HMM(train_total)\n",
    "        true_tags, pred_tags = [], []\n",
    "        for test in test_set:\n",
    "            sentence = [ word for (tag, word) in test ] \n",
    "            labeled_tag = [ tag for (tag, word) in test]\n",
    "            pred_tagsequence = recognize_HMM(cpd_tags, cpd_tagwords, sentence, distinct_tags)   \n",
    "            pred_tags.extend(pred_tagsequence)\n",
    "            true_tags.extend(labeled_tag)\n",
    "            \n",
    "        precision = metrics.precision_score(true_tags, pred_tags, average='macro')\n",
    "        recall = metrics.recall_score(true_tags, pred_tags, average='macro')\n",
    "        F_score = (2 * precision * recall) / ( precision + recall )\n",
    "        print(\"Precision_score:\", precision)\n",
    "        print(\"Recell_score:\", recall)\n",
    "        print(\"F1_score:\", F_score)\n",
    "        print(\"\\n\")        \n",
    "        \n",
    "tagger_words_IN_array = np.array(tagger_words_IN_sents)           # 一定要array\n",
    "test_HMM(tagger_words_IN_array, distinct_tags_IN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结论：\n",
    "区分IN-conj和IN-prep之后，准确率和召回率都**下降**了。两个指标都大代表效果越好。\n",
    "1. 可能因为IN的一些词语既有**conj连词**又有**prep介词**的词性。\n",
    "2. 我区分**IN-conj连词**和**IN-prep介词**的手动规则太简单了，就只根据后面是不是名词来区分。\n",
    "3. 训练时分开，最后测试得出结果时将**conj连词**和**prep介词**合并为IN的标记，再和初始标记对比,下面实现下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stu3/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_score: 0.7652864908578106\n",
      "Recell_score: 0.34423178514289526\n",
      "\n",
      "\n",
      "Precision_score: 0.7557065465642907\n",
      "Recell_score: 0.3330081629957722\n",
      "\n",
      "\n",
      "Precision_score: 0.7771253292642534\n",
      "Recell_score: 0.37182731150473813\n",
      "\n",
      "\n",
      "Precision_score: 0.7441016106681443\n",
      "Recell_score: 0.35094271814353284\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss = ShuffleSplit(n_splits=4, random_state=0, test_size=0.1)\n",
    "\n",
    "for train_index, test_index in ss.split(tagger_words_IN_array):\n",
    "    # 训练IN分开， 测试IN合并\n",
    "    train_set, test_set = tagger_words_IN_array[train_index], tagger_words_array[test_index] \n",
    "    train_total = reduce(operator.add, train_set)\n",
    "    cpd_tags, cpd_tagwords = train_HMM(train_total)\n",
    "    true_tags, pred_tags = [], []\n",
    "    for test in test_set:\n",
    "        sentence = [ word for (tag, word) in test ] \n",
    "        labeled_tag = [ tag for (tag, word) in test]\n",
    "        pred_tagsequence = recognize_HMM(cpd_tags, cpd_tagwords, sentence, distinct_tags)   \n",
    "        pred_tags.extend(pred_tagsequence)\n",
    "        true_tags.extend(labeled_tag)\n",
    "        \n",
    "    pred_tags = [tag if tag != 'IN-prep' and tag != 'IN-conj' else 'IN' for tag in pred_tags]\n",
    "    print(\"Precision_score:\", metrics.precision_score(true_tags, pred_tags, average='macro'))\n",
    "    print(\"Recell_score:\", metrics.recall_score(true_tags, pred_tags, average='macro'))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果依然不如最初没有把IN分开的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引用\n",
    "[^1] [维特比算法实现](https://codingcat.cn/article/16#Recognition%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E6%B3%95)\n",
    "\n",
    "[^2] [准确率和召回率](https://zhuanlan.zhihu.com/p/24322275)\n",
    "\n",
    "[^3] [sklearn.metrics.precision_score()的参数问题](https://stackoverflow.com/questions/52269187/facing-valueerror-target-is-multiclass-but-average-binary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
